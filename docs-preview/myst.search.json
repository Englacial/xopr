{"version":"1","records":[{"hierarchy":{"lvl1":"xopr"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"xopr"},"content":"","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"xopr","lvl2":"Overview"},"type":"lvl2","url":"/#overview","position":2},{"hierarchy":{"lvl1":"xopr","lvl2":"Overview"},"content":"xopr is a Python library designed to make accessing \n\nOpen Polar Radar’s data archives easy, scalable, and reproducible.\n\nWarning\n\nxopr is a work in progress! The API will almost certainly change in the future, so please proceed with caution.\n\nWe welcome your feedback and contributions. If you run into problems or have ideas for how this could be better, please consider \n\nopening an issue. We also welcome pull requests!","type":"content","url":"/#overview","position":3},{"hierarchy":{"lvl1":"xopr","lvl2":"Installing xopr"},"type":"lvl2","url":"/#installing-xopr","position":4},{"hierarchy":{"lvl1":"xopr","lvl2":"Installing xopr"},"content":"For now, xopr is available only directly from source on GitHub. To install xopr, use:pip install xopr\n\nOr, using \n\nuv:uv add xopr","type":"content","url":"/#installing-xopr","position":5},{"hierarchy":{"lvl1":"xopr","lvl2":"Getting Started"},"type":"lvl2","url":"/#getting-started","position":6},{"hierarchy":{"lvl1":"xopr","lvl2":"Getting Started"},"content":"Minimal example of loading and plotting a single frame of radar data:import numpy as np\nimport xopr\n\nopr = xopr.OPRConnection()\n\nframes = opr.load_flight(\"2022_Antarctica_BaslerMKB\", flight_id=\"20221228_01\", data_product=\"CSARP_standard\", max_items=1)\n\n(10*np.log10(frames[0].Data)).plot.imshow(x='slow_time', y='twtt', cmap='gray', yincrease=False)\n\nTo learn more, check out our demo notebooks from the menu on the left side or \n\non GitHub.","type":"content","url":"/#getting-started","position":7},{"hierarchy":{"lvl1":"xopr","lvl2":"Design"},"type":"lvl2","url":"/#design","position":8},{"hierarchy":{"lvl1":"xopr","lvl2":"Design"},"content":"For details on the initial design planning of xopr, please see \n\nthis OPR wiki page.\n\n\n\nxopr acts as an interface to OPR data. It has two primary roles: helping create queries to the OPR STAC catalog to find data and returning radar data in the form of an xarray Dataset.","type":"content","url":"/#design","position":9},{"hierarchy":{"lvl1":"xopr Demo Notebook"},"type":"lvl1","url":"/demo-notebook","position":0},{"hierarchy":{"lvl1":"xopr Demo Notebook"},"content":"This is a basic demonstration of the core features of xopr for loading and plotting radar data.\n\n%load_ext autoreload\n%autoreload 2\n\nimport numpy as np\nimport xarray as xr\nimport hvplot.xarray\nimport geoviews as gv\nimport geoviews.feature as gf\nimport cartopy.crs as ccrs\nimport matplotlib.pyplot as plt\n\nimport xopr\n\nhvplot.extension('bokeh')\n\nYou’ll first establish an OPR session. This object serves to contain any needed information about how to connect to OPR and to the STAC API.\n\nGenerally, you can just use opr = xopr.OPRConnection(), but you may want to customize other options. The most common would be to create a local radar cache, which you can do as shown in the cell below.\n\nIf you specify a cache_dir, then \n\nfsspec will automatically manage a cache of any radar data that you need to download. This makes re-running things fast.\n\n# Establish an OPR session\n# You'll probably want to set a cache directory if you're running this locally to speed\n# up subsequent requests. You can do other things like customize the STAC API endpoint,\n# but you shouldn't need to do that for most use cases.\nopr = xopr.OPRConnection(cache_dir=\"radar_cache\")\n\n# Or you can open a connection without a cache directory (for example, if you're parallelizing\n# this on a cloud cluster without persistent storage).\n#opr = xopr.OPRConnection()\n\nIn the STAC catalog, every season (an entity such as 2022_Antarctica_BaslerMKB) is a distinct collection. You can list the available collections.\n\nWarning\n\nWe haven’t loaded the entire data catalog into our STAC catalog yet. It’s still just a small testing set. So keep in mind that the seasons you can access (for now) are limited.\n\n# List the available OPR datasets\ncollections = opr.get_collections()\nprint([c['id'] for c in collections])\nselected_collection = '2022_Antarctica_BaslerMKB'  # Select a collection for demonstration\nprint(f\"Selected collection: {selected_collection}\")\n\nSimilarly, you can list available flights for a given season. When we add a season into the STAC catalog, we add every flight for which a CSARP_standard product is available. (And we also link to other data products, if those are available.)\n\n# List flights in the selected collection\nflights = opr.get_flights(selected_collection)\nprint(f\"Found {len(flights)} flights in collection {selected_collection}\")\nprint(f\"The first 3 flights are: {[f['flight_id'] for f in flights[:3]]}\")\n\nOnce you pick a flight, we can actually start loading data. How much data this needs to transfer will vary depending on both how long the flight is and how the underlying data is stored.\n\nWe’re working on migrating OPR data files to be cloud-optimized, however most of them aren’t and some of them are old-school MATLAB v5 files. xopr is designed to hide these differences from you as much as possible, but that can only go so far.\n\nselected_flight = '20230109_01' # Or from the list of flights like this: flights[0]['flight_id']\nprint(f\"Selected flight: {selected_flight}\")\n\nstac_items = opr.query_frames(seasons=[selected_collection], flight_ids=[selected_flight])\nframes = opr.load_frames(stac_items)\n\nLet’s look a single frame. This corresponds to a single .mat file that you might download from the OPR website. The structure of this should look familiar to you.\n\nIf you’ve tried directly loading one of these files in Python, you’ll probably know that there are some quirks. We try to handle those behind the scenes and give you a nicely-formatted \n\nxarray Dataset.\n\nIf you’ve never heard of xarray, this might be a good time to go read the \n\nxarray overview doc.\n\nTip\n\nWe’re aspirationally aiming for this representation to be CF-compliant. We’re not there yet, but that’s the goal. If you’ve never heard of CF-compliance, then you can safely ignore this and you probably don’t care. But if you’re curious, you can read about \n\nCF Metadata Conventions.\n\nTip\n\nxOPR produces datasets with very long nested attributes. In order to provide a nicer notebook preview of them, we’ve included an xarray accessor that provides an improved _repr_html_ function. If you want nice previews of nested dictionaries of attributes, you can use:\n\nradar_ds.xopr\n\n# Inspect an individual frame\nframes[0].xopr\n\nMerging frames together to get a full flight line is easy with built-in xarray functions.\n\nTip\n\nYou can also pass merge_flights=True to load_frames and you’ll get back a list with one dataset per flight. :::\n\nfrom xopr.util import merge_dicts_no_conflicts\n\n# Combine the frames into a single xarray Dataset representing the flight line\nflight_line = xr.concat(frames, dim='slow_time', combine_attrs=merge_dicts_no_conflicts).sortby('slow_time')\nflight_line.xopr\n\nThis is an example of one way to make a plot of where this flight line is. This plot is \n\nBokeh-based, so it’s fully interactive. Use the tools on the right to pan and zoom.\n\n# Plot a map of where the data was collected with a basemap on an EPSG:3031 projection\nproj = ccrs.Stereographic(central_latitude=-90, true_scale_latitude=-71)\ncoords = flight_line[['Longitude', 'Latitude']].to_dataframe().dropna()\nline_plot = gv.Points([coords.values]).opts(size=1)\nstart_point = gv.Points([coords.iloc[0].values]).opts(color='green', size=5, projection=proj)\n(gf.ocean * gf.coastline * line_plot * start_point).opts(projection=proj, aspect='equal')\n\nAnother common radar operation is stacking. Depending on our goals, we might want to do that in one of a couple of ways.\n\nIf we want to stack every 10 traces, we can do something like this:stacked = flight_line.Data.rolling(slow_time=10, center=True).mean()\n\nIf our goal is to display a radargram, it might be more useful to average over fixed-time windows, which we can do like this:stacked = flight_line.resample(slow_time='2s').mean()\n\nThe advantage of the latter approach is that you end up with a uniform spacing in the slow_time dimension. This allows for more efficient plotting (imshow can only be used with fixed spacing -- try pcolormesh if your spacing is variable).\n\n#stacked = flight_line.Data.rolling(slow_time=10, center=True).mean()\nstacked = flight_line.resample(slow_time='2s').mean()\npwr_dB = 10*np.log10(np.abs(stacked.Data))\n\nOPR data often also includes traced layers (surface, bed, and occasionally internal layers). There are two distinct formats that OPR uses, which you may be familiar with from the CReSIS imb.picker tool. One is a database that stores layer picks. The other is a layer file that is available as a separate data product.\n\nxopr allows you to fetch the relevant layer information from either source. In the cell below, we first try to load from the OPS database. If that fails (because there is no data for the currently selected flight) then we load the layer files. In the background, the former works by querying the OPS API and the latter uses thes STAC catalog.\n\nOnce again, we do our best to hide the different formats. Once you’ve loaded the layers either way, you’ll get a dictionary mapping layer IDs to xarray Datasets of the same basic structure.\n\nTip\n\nHow to handle layers is still very much under development. We would love feedback on this topic.\n\nstacked.xopr\n\nlayers = None\ntry:\n    layers = opr.get_layers_db(stacked)  # Fetch layers from the database\nexcept Exception as e:\n    print(f\"Error fetching layers: {e}\")\n    print(\"Trying to load layers from file instead...\")\n\n    layers = opr.get_layers_files(stacked)\n\nlayers[1] # Display the surface layer as an example\n\nFinally, let’s make a radargram. If we were successful in loading layers, we will also plot the surface and bed layers here.\n\nTip\n\nimshow is a very fast way of plotting regulary-spaced 2D data. It works great if your slow_time spacing is uniform, as it is here because we already resampled during stacking. If you have non-uniformly spaced data, you must use pcolormesh or another plotting tool that can handle non-uniformly spaced data.\n\nfig, ax = plt.subplots(figsize=(15, 4))\npwr_dB.plot.imshow(x='slow_time', cmap='gray', ax=ax)\nax.invert_yaxis()\n\nif layers:\n    layers[1]['twtt'].plot(ax=ax, x='slow_time', linestyle=':', label='Surface')\n    layers[2]['twtt'].plot(ax=ax, x='slow_time', linestyle='--', label='Bed')\n    ax.legend()\n\nOPR data comes from many sources. Collecting and processing that data has required the contributions of many people over multiple decades. We want to make it easy for you, as a user, to figure out how to appropriately cite the data you’ve used. This is still just an early prototype, but here’s an idea of what it might look like to generate a report on how to cite your data:\n\nprint(opr.generate_citation(flight_line))","type":"content","url":"/demo-notebook","position":1},{"hierarchy":{"lvl1":"Reading parameters"},"type":"lvl1","url":"/reading-params","position":0},{"hierarchy":{"lvl1":"Reading parameters"},"content":"Data products exported from Open Polar Radar contain a huge amount of metadata, stored primarily in param_<processing step> structures. These are initially encoded in spreadsheets, which may be found in the \n\nopr_params git repository and follow the schema described in the \n\nParameter Spreadsheet Guide and the \n\nParameter Spreadsheets section of the OPR Toolbox Guide.\n\nWhen data is processed, the parameters used for each processing stage are saved with the exported data file. xOPR will load as much of this data as possible and make it available as attributes of the returned datasets.\n\nGenerally, all files you load should have a param_records attribute that encodes the parameter spreadsheet at the point where the raw data was broken into frames. Depending on the processing level you load, you will also find parameter dictionaries for later processing steps. For example, CSARP_standard data products should have a param_sar dictionary and CSARP_qlook products should have a param_qlook dictionary.\n\n(For a reference to the processing stages, see \n\nProcessing Steps on the OPR wiki.)\n\n%load_ext autoreload\n%autoreload 2\n\nimport xarray as xr\n\nimport xopr\n\n# Establish an OPR session\n# You'll probably want to set a cache directory if you're running this locally to speed\n# up subsequent requests. You can do other things like customize the STAC API endpoint,\n# but you shouldn't need to do that for most use cases.\nopr = xopr.OPRConnection(cache_dir=\"radar_cache\")\n\n# Or you can open a connection without a cache directory (for example, if you're parallelizing\n# this on a cloud cluster without persistent storage).\n#opr = xopr.OPRConnection()\n\nseason, flight_id = '2022_Antarctica_BaslerMKB', '20230109_01'\n#season, flight_id = '2016_Antarctica_DC8', '20161117_06'\nprint(f\"Selected flight: {flight_id} from season {season}\")\n\nstac_items = opr.query_frames(seasons=[season], flight_ids=[flight_id], max_items=2)\nframes = opr.load_frames(stac_items, data_product='CSARP_standard')\nexample_frame = frames[0]\n\nTip\n\nxOPR produces datasets with very long nested attributes. In order to provide a nicer notebook preview of them, we’ve included an xarray accessor that provides an improved _repr_html_ function. If you want nice previews of nested dictionaries of attributes, you can use:\n\nradar_ds.xopr\n\nexample_frame.xopr\n\nThe structure of each parameter dictionary should be the same (see \n\nParameter Spreadsheet in the OPR Toolbox Guide). The multiple parameter dictionaries encode the history of the parameters throughout the processing. Generally, if you’re looking for a parameter that is fundamental to the radar instrument, it shouldn’t matter which structure you read since this won’t change. For example, here we’re reading the start frequency of the chirp from param_records and param_sar and getting the same value.\n\nprint(example_frame.attrs['param_records']['radar']['wfs']['f0'])\nprint(example_frame.attrs['param_sar']['radar']['wfs']['f0'])\n\nIf you’re looking for a parameter specific to the post-processing, it’s generally best to use the parameter structure associated with the level of processing you’re working with.\n\nprint(example_frame.attrs['param_sar']['sar']['chunk_len'])\n\nThere are some parts of the parameter structure that we can’t really load correctly. Matlab function handles will show up like this, but there’s not much you can actually do with this:\n\nexample_frame.attrs['param_records']['radar']['lever_arm_fh']\n\nPossible discrepancies between legacy Matlab and HDF5 files\n\nUnfortunatley, the OPR data is a mix of legacy Matlab file formats and Matlab-generated HDF5 files. These files need to be loaded through different code paths, so there may be discrepancies in how some of the parameters are handled. If you find any issues that are causing problems for your use case, please \n\nopen an issue to let us know.\n\nWhen you combine multiple frames together, you can use xOPR’s xopr.util.merge_dicts_no_conflicts to combine attributes. This helper function will navigate through the nested dictionaries and keep only the parameters that are common across all input frames.\n\nfrom xopr.util import merge_dicts_no_conflicts\nxr.concat(frames, dim='slow_time', combine_attrs=merge_dicts_no_conflicts).sortby('slow_time').xopr\n\nOf course, if you don’t want to remember all that, you can just directly call the xOPR helper function for merging flights:\n\nopr.merge_flights_from_frames(frames)[0].xopr","type":"content","url":"/reading-params","position":1},{"hierarchy":{"lvl1":"Searching by region and scaling up"},"type":"lvl1","url":"/search-and-scaling","position":0},{"hierarchy":{"lvl1":"Searching by region and scaling up"},"content":"This notebook demonstrates:\n\nLoading radar frames intersecting a geographic region (in this case an ice shelf)\n\nSimple processing on focused radar data\n\nHow to use \n\nDask clusters to scale up your analysis, optionally using \n\nCoiled to parallelize your analysis in the cloud\n\n%load_ext autoreload\n%autoreload 2\n\nimport numpy as np\nimport xarray as xr\nimport hvplot.xarray\nimport geoviews as gv\nimport geoviews.feature as gf\nimport cartopy.crs as ccrs\nimport matplotlib.pyplot as plt\nimport shapely\nimport scipy.constants\nimport pandas as pd\nimport traceback\n\nimport xopr.opr_access\nimport xopr.geometry\n\nhvplot.extension('bokeh')\n\n# Useful projections\nepsg_3031 = ccrs.Stereographic(central_latitude=-90, true_scale_latitude=-71)\nlatlng = ccrs.PlateCarree()\nfeatures = gf.ocean.options(scale='50m').opts(projection=epsg_3031) * gf.coastline.options(scale='50m').opts(projection=epsg_3031)\n\n# Establish an OPR session\n# You'll probably want to set a cache directory if you're running this locally to speed\n# up subsequent requests. You can do other things like customize the STAC API endpoint,\n# but you shouldn't need to do that for most use cases.\nopr = xopr.opr_access.OPRConnection(cache_dir=\"radar_cache\")\n\n# Or you can open a connection without a cache directory (for example, if you're parallelizing\n# this on a cloud cluster without persistent storage).\n#opr = xopr.OPRConnection()\n\nxopr includes a helper module xopr.geometry with some useful utilities. You can call get_antarctic_regions to select one or more regions from the \n\nMEaSUREs Antarctic Boundaries dataset.\n\nBefore we dive in, let’s look at a few examples. Note that the GeoJSON returned is EPSG:4326 (WGS84), so they look a bit weird. In the examples below, we reproject them to a more familiar EPSG:3031 for the previews.\n\nYou can select any (combination) of the three regions: Peninsula, West, and East like this:\n\ntmp = xopr.geometry.get_antarctic_regions(regions='West', merge_regions=True)\nxopr.geometry.project_geojson(tmp)\n\nThe MEaSUREs dataset categorizes regions as GR (grounded), FL (floating), or IS (island). You can select by those:\n\ntmp = xopr.geometry.get_antarctic_regions(type='FL', merge_regions=True)\nxopr.geometry.project_geojson(tmp)\n\nYou can also select by name. Note that you can pass a list to any of these parameters, as shown here.\n\nBy default merge_regions=True and the return value is a single GeoJSON geometry. If you set merge_regions=False, you’ll instead get back a GeoDataFrame with information about the regions included.\n\nTip\n\nIf you’re trying to figure out what parameters to use to find your region, the \n\nuser guide for the MEaSUREs data product may be useful.\n\nQuantarctica also includes this layer under the “glaciology” category if you want to be able to browse around and see the regions.\n\nxopr.geometry.get_antarctic_regions(name=['LarsenD', 'LarsenE'], merge_regions=False)\n\nFor the rest of this notebook, we’re going to look at just the Dotson ice shelf, which we select as follows:\n\nregion = xopr.geometry.get_antarctic_regions(name=\"Dotson\", type=\"FL\", merge_regions=True)\nxopr.geometry.project_geojson(region)\n\nHere we see a new way of searching for radar frames by geometry. Simply passing our GeoJSON region gives us every available frame that intersects it.\n\ndata_product controls what you get back. If you set this to CSARP_standard (or another CSARP_ standard data product), you’ll get back a list of loaded radar frames. If you set it to None, you’ll get back the actual STAC items. That’s helpful here because we’re going to distribute the process of analyzing these frames, so we don’t actually want to load any data just yet.\n\nPassing max_items=10 limits the search to a maximum of 10 items to keep this running quickly on the GitHub Actions runners, but feel free to experiment with removing it.\n\n#stac_items = opr.search_by_geometry(region, data_product=None, max_items=10)\nstac_items = opr.query_frames(geometry=region, max_items=10)\n\nPlotting a map should look familiar. Here we’ve also overlaid the region we’re searching.\n\nYou’ll notice that the region doesn’t quite line up with the GeoViews coastline feature. That’s expected. The coastline feature is a relatively low resolution global data product that you shouldn’t treat as a real grounding line.\n\n# Plot a map of our loaded data over the selected region on an EPSG:3031 projection\n\n# Create a GeoViews object for the selected region\nregion_gv = gv.Polygons(region, crs=latlng).opts(\n    color='green',\n    line_color='black',\n    fill_alpha=0.5,\n    projection=epsg_3031,\n)\n# Plot the frame geometries\nframe_lines = []\nfor item in stac_items:\n    path_data = [tuple(coord) for coord in item['geometry']['coordinates']]\n    frame_lines.append(gv.Path([path_data], crs=latlng).opts(\n        line_width=2,\n        projection=epsg_3031\n    ))\n\n(features * region_gv * gv.Overlay(frame_lines)).opts(projection=epsg_3031)\n\nNow that we’ve picked out some data, we’re going to define some functions to do some simple analysis on it. We won’t explain every detail of the code below, but basically it’s picking out the surface and bed reflection powers and giving us back a Dataset with those powers in decibels.\n\ndef extract_layer_peak_power(radar_ds, layer_twtt, margin_twtt):\n    \"\"\"\n    Extract the peak power of a radar layer within a specified margin around the layer's two-way travel time (TWTT).\n\n    Parameters:\n    - radar_ds: xarray Dataset containing radar data.\n    - layer_twtt: The two-way travel time of the layer to extract.\n    - margin_twtt: The margin around the layer's TWTT to consider for peak power extraction.\n\n    Returns:\n    - A DataArray containing the peak power values for the specified layer.\n    \"\"\"\n    \n    # Ensure that layer_twtt.slow_time matches the radar_ds slow_time\n    t_start = np.minimum(radar_ds.slow_time.min(), layer_twtt.slow_time.min())\n    t_end = np.maximum(radar_ds.slow_time.max(), layer_twtt.slow_time.max())\n    layer_twtt = layer_twtt.sel(slow_time=slice(t_start, t_end))\n    radar_ds = radar_ds.sel(slow_time=slice(t_start, t_end))\n    #layer_twtt = layer_twtt.interp(slow_time=radar_ds.slow_time, method='nearest')\n    layer_twtt = layer_twtt.reindex(slow_time=radar_ds.slow_time, method='nearest', tolerance=pd.Timedelta(seconds=1), fill_value=np.nan)\n    \n    # Calculate the start and end TWTT for the margin\n    start_twtt = layer_twtt - margin_twtt\n    end_twtt = layer_twtt + margin_twtt\n    \n    # Extract the data within the specified TWTT range\n    data_within_margin = radar_ds.where((radar_ds.twtt >= start_twtt) & (radar_ds.twtt <= end_twtt), drop=True)\n\n    power_dB = 10 * np.log10(np.abs(data_within_margin.Data))\n\n    # Find the twtt index corresponding to the peak power\n    peak_twtt_index = power_dB.argmax(dim='twtt')\n    # Convert the index to the actual TWTT value\n    peak_twtt = power_dB.twtt[peak_twtt_index]\n\n    # Calculate the peak power in dB\n    peak_power = power_dB.isel(twtt=peak_twtt_index)\n\n    # Remove unnecessary dimensions\n    peak_twtt = peak_twtt.drop_vars('twtt')\n    peak_power = peak_power.drop_vars('twtt')\n    \n    return peak_twtt, peak_power\n\ndef surface_bed_reflection_power(stac_item, opr=xopr.opr_access.OPRConnection()):\n\n    frame = opr.load_frame(stac_item, data_product='CSARP_standard')\n    frame = frame.resample(slow_time='5s').mean()\n\n    try:\n        layers = opr.get_layers_db(frame)\n    except Exception as e:\n        print(f\"Error retrieving layers: {e}\")\n        return None\n    \n    # Re-pick surface and bed layers to ensure we're getting the peaks\n    speed_of_light_in_ice = scipy.constants.c / np.sqrt(3.17)  # Speed of light in ice (m/s)\n    layer_selection_margin_twtt = 50 / speed_of_light_in_ice # approx 50 m margin in ice\n    surface_repicked_twtt, surface_power = extract_layer_peak_power(frame, layers[1]['twtt'], layer_selection_margin_twtt)\n    bed_repicked_twtt, bed_power = extract_layer_peak_power(frame, layers[2]['twtt'], layer_selection_margin_twtt)\n\n    # Create a dataset from surface_repicked_twtt, bed_repicked_twtt, surface_power, and bed_power\n\n    reflectivity_dataset = xr.merge([\n        surface_repicked_twtt.rename('surface_twtt'),\n        bed_repicked_twtt.rename('bed_twtt'),\n        surface_power.rename('surface_power_dB'),\n        bed_power.rename('bed_power_dB'),\n        ])\n\n    flight_line_metadata = frame.drop_vars(['Data', 'Surface', 'Bottom'])\n    reflectivity_dataset = xr.merge([reflectivity_dataset, flight_line_metadata])\n\n    reflectivity_dataset = reflectivity_dataset.drop_dims(['twtt'])  # Remove the twtt dimension since everything has been flattened\n\n    attributes_to_copy = ['season', 'segment', 'doi', 'ror', 'funder_text']\n    reflectivity_dataset.attrs = {attr: frame.attrs[attr] for attr in attributes_to_copy if attr in frame.attrs}\n\n    return reflectivity_dataset\n\nLet’s try out our analysis function on one frame. The plot below shows surface and bed power for a single frame of radar data.\n\nreflectivity = surface_bed_reflection_power(stac_items[0], opr=opr)\n\nfig, ax = plt.subplots(figsize=(8, 4))\nreflectivity['surface_power_dB'].plot(ax=ax, x='slow_time', label='Surface')\nreflectivity['bed_power_dB'].plot(ax=ax, x='slow_time', label='Bed')\nax.set_ylabel('Power [dB]')\nax.legend()\nplt.show()\n\nOk, that works, so now let’s scale up. You could just write a loop, but this is a parallelizable task, so we want to share the load out across whatever computational resources you might have.\n\nThis is good time to pause and note a few things:\n\nThis is getting too complicated!\n\nWe’re about to jump into how to parallelize your workflows. This part is completely optional. If you’re happily doing your analysis without thinking about this, you can feel free to ignore the rest of the tutorial.\n\nBut if you’re curious or you’d like to run things faster...\n\nIf you’re still with us, this would probably be a good time to briefly read up on \n\nDask. In short, Dask gives us a way to write code that looks pretty close to standard Python code but also easily distribute jobs across available compute infrastructure.\n\nThe defaults in this notebook will simply parallelize this workflow across however many CPU cores you have on your machine. We’ll also discuss using \n\nCoiled to scale your workflows into the cloud.\n\nThere are many more options, which you can read about in \n\nthe Dask documentation. These options allow you to distribute your workflow across various cloud services or using any HPC resources you may have acesss to.\n\nGetting started, we’ll create a Dask LocalCluster, which is just going to let us distribute the workload across the CPUs on our machine:\n\nimport dask\nfrom dask.distributed import LocalCluster\n\nclient = LocalCluster().get_client()\n\nWe could have alternatively created any cluster we want. For example, this is how you’d create a Coiled cluster:\n\n# import dask\n# import coiled\n# cluster = coiled.Cluster(n_workers=10)\n# client = cluster.get_client()\n\nNow we’re going to use client.map to run our function across whatever resources are in the client object.\n\nNote that if you’re using a cloud cluster, you probably don’t want to pass your local opr object. Since you don’t have shared storage anyway, it’ll be better to let it load a default OPRConnection() object on each worker.\n\nfutures = client.map(surface_bed_reflection_power, stac_items, opr=opr)\n\n# Process results as they complete, capturing exceptions\nresults = []\nfor future in dask.distributed.as_completed(futures):\n    try:\n        result = future.result()\n        results.append(result)\n    except Exception as e:\n        print(traceback.format_exc())\n\nGreat! If that ran successfully, results should now be a list of Dataset’s. Now we’re back to normal code to visualize our results.\n\n# Create a GeoViews object for the selected region\nregion_gv = gv.Polygons(region, crs=latlng).opts(\n    line_color='black',\n    fill_alpha=0,\n    projection=epsg_3031,\n)\n\ndata_lines = []\nfor ds in results:\n    ds['bed_minus_surf'] = ds['bed_power_dB'] - ds['surface_power_dB']\n    ds = ds.dropna(dim='slow_time')\n    ds = xopr.geometry.project_dataset(ds, target_crs='EPSG:3031')\n    sc = ds.hvplot.scatter(x='x', y='y', c='bed_minus_surf',\n                           hover_cols=['surface_power_dB', 'bed_power_dB'],\n                           cmap='turbo', size=3)\n    data_lines.append(sc)\n\n(features * region_gv * gv.Overlay(data_lines)).opts(aspect='equal')\n\nIf you’re using a cloud or HPC cluster, it’s good practice to explicitly close it. In most cases, it’ll have an auto-timeout, but we’ll just close ours to be safe.\n\n#cluster.close()\n\nCongrats! If you made it this far, you’ve learned how to parallelize your radar analysis workflows!","type":"content","url":"/search-and-scaling","position":1},{"hierarchy":{"lvl1":"Test Polar Map Integration"},"type":"lvl1","url":"/test-polar-map","position":0},{"hierarchy":{"lvl1":"Test Polar Map Integration"},"content":"This notebook demonstrates how to embed interactive polar maps with GeoParquet data in Jupyter Book.\n\n","type":"content","url":"/test-polar-map","position":1},{"hierarchy":{"lvl1":"Test Polar Map Integration","lvl2":"Antarctic Map Example"},"type":"lvl2","url":"/test-polar-map#antarctic-map-example","position":2},{"hierarchy":{"lvl1":"Test Polar Map Integration","lvl2":"Antarctic Map Example"},"content":"Below is an interactive map showing test data from Antarctica. The data is loaded from a GeoParquet file that contains a random walk path starting from the South Pole.\n\n<iframe \n    src=\"../_static/maps/polar.html\" \n    width=\"100%\" \n    height=\"600\"\n    frameborder=\"0\"\n    style=\"border: 1px solid #ccc; border-radius: 5px;\"\n    onload=\"this.contentWindow.CONFIG = {pole: 'south', parquetFiles: ['test_antarctic_random_walk.parquet'], defaultZoom: 3}\">\n</iframe><iframe \n    src=\"../_static/maps/polar.html\" \n    width=\"100%\" \n    height=\"600\"\n    frameborder=\"0\"\n    style=\"border: 1px solid #ccc; border-radius: 5px;\"\n    onload=\"this.contentWindow.CONFIG = {pole: 'south', parquetFiles: ['test_antarctic_random_walk.parquet'], defaultZoom: 3}\">\n</iframe>\n\n","type":"content","url":"/test-polar-map#antarctic-map-example","position":3},{"hierarchy":{"lvl1":"Test Polar Map Integration","lvl2":"Features"},"type":"lvl2","url":"/test-polar-map#features","position":4},{"hierarchy":{"lvl1":"Test Polar Map Integration","lvl2":"Features"},"content":"The map above includes:\n\nInteractive navigation: Zoom and pan to explore the data\n\nClick for details: Click on any feature to see its name and description\n\nCoordinate display: Click on empty areas to see lat/lon coordinates\n\nGeoParquet support: Data loaded directly from Parquet files using WebAssembly","type":"content","url":"/test-polar-map#features","position":5},{"hierarchy":{"lvl1":"Test Polar Map Integration","lvl2":"Data Format"},"type":"lvl2","url":"/test-polar-map#data-format","position":6},{"hierarchy":{"lvl1":"Test Polar Map Integration","lvl2":"Data Format"},"content":"The GeoParquet file contains:\n\nA LineString showing a random walk path from the South Pole\n\nPoint features marking the start and end points\n\nMetadata including names and descriptions for each feature\n\n","type":"content","url":"/test-polar-map#data-format","position":7},{"hierarchy":{"lvl1":"Test Polar Map Integration","lvl2":"Arctic Map Example"},"type":"lvl2","url":"/test-polar-map#arctic-map-example","position":8},{"hierarchy":{"lvl1":"Test Polar Map Integration","lvl2":"Arctic Map Example"},"content":"You can also display Arctic data by changing the pole parameter to 'north':CONFIG = {\n    pole: 'north',\n    parquetFiles: ['arctic_data.parquet'],\n    defaultZoom: 3\n}\n\n","type":"content","url":"/test-polar-map#arctic-map-example","position":9},{"hierarchy":{"lvl1":"Test Polar Map Integration","lvl2":"Python Integration"},"type":"lvl2","url":"/test-polar-map#python-integration","position":10},{"hierarchy":{"lvl1":"Test Polar Map Integration","lvl2":"Python Integration"},"content":"You can also generate maps dynamically from Python:\n\nfrom IPython.display import IFrame, HTML\nimport json\n\ndef create_polar_map(pole='south', parquet_files=None, height=600):\n    \"\"\"\n    Create an embedded polar map.\n    \n    Parameters:\n    -----------\n    pole : str\n        'north' or 'south' for Arctic or Antarctic projection\n    parquet_files : list\n        List of parquet file paths to display\n    height : int\n        Height of the map in pixels\n    \"\"\"\n    if parquet_files is None:\n        parquet_files = ['test_antarctic_random_walk.parquet']\n    \n    config = {\n        'pole': pole,\n        'parquetFiles': parquet_files,\n        'defaultZoom': 3\n    }\n    \n    # Create HTML with embedded configuration\n    html = f'''\n    <iframe \n        src=\"../_static/maps/polar.html\"\n        width=\"100%\" \n        height=\"{height}\"\n        frameborder=\"0\"\n        style=\"border: 1px solid #ccc; border-radius: 5px;\"\n        onload=\"this.contentWindow.CONFIG = {json.dumps(config)}\">\n    </iframe>\n    '''\n    \n    return HTML(html)\n\n# Display the map\ncreate_polar_map(pole='south', height=500)\n\n","type":"content","url":"/test-polar-map#python-integration","position":11},{"hierarchy":{"lvl1":"Test Polar Map Integration","lvl2":"Technical Details"},"type":"lvl2","url":"/test-polar-map#technical-details","position":12},{"hierarchy":{"lvl1":"Test Polar Map Integration","lvl2":"Technical Details"},"content":"","type":"content","url":"/test-polar-map#technical-details","position":13},{"hierarchy":{"lvl1":"Test Polar Map Integration","lvl3":"Projections Used","lvl2":"Technical Details"},"type":"lvl3","url":"/test-polar-map#projections-used","position":14},{"hierarchy":{"lvl1":"Test Polar Map Integration","lvl3":"Projections Used","lvl2":"Technical Details"},"content":"Antarctic: EPSG:3031 (WGS 84 / Antarctic Polar Stereographic)\n\nArctic: EPSG:3413 (WGS 84 / NSIDC Sea Ice Polar Stereographic North)","type":"content","url":"/test-polar-map#projections-used","position":15},{"hierarchy":{"lvl1":"Test Polar Map Integration","lvl3":"Base Maps","lvl2":"Technical Details"},"type":"lvl3","url":"/test-polar-map#base-maps","position":16},{"hierarchy":{"lvl1":"Test Polar Map Integration","lvl3":"Base Maps","lvl2":"Technical Details"},"content":"NASA GIBS Blue Marble imagery\n\nServed via WMS with proper polar projections","type":"content","url":"/test-polar-map#base-maps","position":17},{"hierarchy":{"lvl1":"Test Polar Map Integration","lvl3":"Technology Stack","lvl2":"Technical Details"},"type":"lvl3","url":"/test-polar-map#technology-stack","position":18},{"hierarchy":{"lvl1":"Test Polar Map Integration","lvl3":"Technology Stack","lvl2":"Technical Details"},"content":"OpenLayers: For map rendering and interaction\n\nparquet-wasm: WebAssembly module for reading Parquet files in the browser\n\nApache Arrow: For parsing the Arrow IPC format from parquet-wasm\n\nproj4js: For coordinate transformations","type":"content","url":"/test-polar-map#technology-stack","position":19},{"hierarchy":{"lvl1":"Test Polar Map - Working Version"},"type":"lvl1","url":"/test-polar-map-working","position":0},{"hierarchy":{"lvl1":"Test Polar Map - Working Version"},"content":"This page demonstrates the polar map integration with GeoParquet data.","type":"content","url":"/test-polar-map-working","position":1},{"hierarchy":{"lvl1":"Test Polar Map - Working Version","lvl2":"Map Display"},"type":"lvl2","url":"/test-polar-map-working#map-display","position":2},{"hierarchy":{"lvl1":"Test Polar Map - Working Version","lvl2":"Map Display"},"content":"Below is an Antarctic map showing test data. The map loads GeoParquet files directly in the browser using WebAssembly.","type":"content","url":"/test-polar-map-working#map-display","position":3},{"hierarchy":{"lvl1":"Test Polar Map - Working Version","lvl2":"How It Works"},"type":"lvl2","url":"/test-polar-map-working#how-it-works","position":4},{"hierarchy":{"lvl1":"Test Polar Map - Working Version","lvl2":"How It Works"},"content":"GeoParquet Loading: The map uses parquet-wasm to read Parquet files directly in the browser\n\nProjection: Uses EPSG:3031 for Antarctic Polar Stereographic projection\n\nBasemap: NASA GIBS Blue Marble imagery via WMS\n\nInteraction: Click features for details, click empty areas for coordinates","type":"content","url":"/test-polar-map-working#how-it-works","position":5},{"hierarchy":{"lvl1":"Test Polar Map - Working Version","lvl2":"Data"},"type":"lvl2","url":"/test-polar-map-working#data","position":6},{"hierarchy":{"lvl1":"Test Polar Map - Working Version","lvl2":"Data"},"content":"The example shows:\n\nA random walk path from the South Pole (LineString)\n\nStart and end points (Point features)\n\nFeature properties with names and descriptions","type":"content","url":"/test-polar-map-working#data","position":7},{"hierarchy":{"lvl1":"Test Polar Map - Working Version","lvl2":"Configuration"},"type":"lvl2","url":"/test-polar-map-working#configuration","position":8},{"hierarchy":{"lvl1":"Test Polar Map - Working Version","lvl2":"Configuration"},"content":"The map can be configured with:\n\npole: ‘north’ or ‘south’ for Arctic/Antarctic\n\nparquetFiles: Array of parquet file paths to load\n\ndefaultZoom: Initial zoom level","type":"content","url":"/test-polar-map-working#configuration","position":9},{"hierarchy":{"lvl1":"Test Polar Map - Working Version","lvl2":"Technical Stack"},"type":"lvl2","url":"/test-polar-map-working#technical-stack","position":10},{"hierarchy":{"lvl1":"Test Polar Map - Working Version","lvl2":"Technical Stack"},"content":"OpenLayers for map rendering and interaction\n\nparquet-wasm for reading Parquet files via WebAssembly\n\nApache Arrow for parsing Arrow IPC format\n\nproj4js for coordinate transformations","type":"content","url":"/test-polar-map-working#technical-stack","position":11}]}